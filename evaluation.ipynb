{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import pickle as pkl\n",
    "import random\n",
    "import math\n",
    "transform_test = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize(mean=(0.5,), std=(0.5,))]\n",
    "        )\n",
    "\n",
    "class get_dataset(Dataset):\n",
    "    def __init__(self, h5_file_path):    \n",
    "        self.h5_file_path = h5_file_path\n",
    "        with h5py.File(self.h5_file_path, 'r') as data_file:\n",
    "            self.images = np.array(data_file['data']) \n",
    "            self.labels = np.array(data_file['label']) \n",
    "           \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        label = torch.tensor(self.labels[item])\n",
    "        image = np.array(self.images[item, :, :, :])\n",
    "        image = np.transpose(image, (1, 2, 0))  \n",
    "        image = transform_test(image)\n",
    "    \n",
    "        return [image, label]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset, TensorDataset\n",
    "from torch.utils.data import Subset\n",
    "import torchvision\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "def calculate_auc(list_a, list_b):\n",
    "    l1,l2 = len(list_a),len(list_b)\n",
    "    y_true,y_score = [],[]\n",
    "    for i in range(l1):\n",
    "        y_true.append(0)\n",
    "    for i in range(l2):\n",
    "        y_true.append(1)\n",
    "    y_score.extend(list_a)\n",
    "    y_score.extend(list_b)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "\n",
    "class FeatureHook():\n",
    "\n",
    "    def __init__(self, module):\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "\n",
    "\n",
    "def correlation(m,n):\n",
    "    m = F.normalize(m,dim=-1)\n",
    "    n = F.normalize(n,dim=-1).transpose(0,1)\n",
    "    cose = torch.mm(m,n)\n",
    "    matrix = 1-cose\n",
    "    matrix = matrix/2\n",
    "    return matrix\n",
    "\n",
    "def pairwise_euclid_distance(A):\n",
    "    sqr_norm_A = torch.unsqueeze(torch.sum(torch.pow(A, 2),dim=1),dim=0)\n",
    "    sqr_norm_B = torch.unsqueeze(torch.sum(torch.pow(A, 2), dim=1), dim=1)\n",
    "    inner_prod = torch.matmul(A, A.transpose(0,1))\n",
    "    tile1 = torch.reshape(sqr_norm_A,[A.shape[0],1])\n",
    "    tile2 = torch.reshape(sqr_norm_B,[1,A.shape[0]])\n",
    "    return tile1+tile2 - 2*inner_prod\n",
    "\n",
    "\n",
    "def correlation_dist(A):\n",
    "    A = F.normalize(A,dim=-1)\n",
    "    cor = pairwise_euclid_distance(A)\n",
    "    cor = torch.exp(-cor)\n",
    "\n",
    "    return cor\n",
    "\n",
    "\n",
    "def cal_cor(model,dataloader):\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for i,(x,y) in enumerate(dataloader):\n",
    "            x, y = x.float().cuda(), y.cuda()\n",
    "            output = model(x.float())\n",
    "            outputs.append(output.cpu().detach())\n",
    "\n",
    "    output = torch.cat(outputs,dim=0)\n",
    "    cor_mat = correlation(output,output)\n",
    "\n",
    "\n",
    "    model = model.cpu()\n",
    "    return cor_mat\n",
    "\n",
    "def output_to_label(output):\n",
    "    shape = output.shape\n",
    "    pred = torch.argmax(output,dim=1)\n",
    "\n",
    "    preds = 0.01 * torch.ones(shape)\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        preds[i,pred[i]]=1\n",
    "\n",
    "    preds = torch.softmax(preds,dim=-1)\n",
    "\n",
    "    print(preds[0,:])\n",
    "    return preds\n",
    "\n",
    "def cal_cor_onehot(model, dataloader):\n",
    "    model.eval()\n",
    "    model = model.cuda()\n",
    "    preds = []\n",
    "    for x,y in dataloader:\n",
    "        x = x.float().cuda()\n",
    "        output = model(x)\n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        preds.append(pred.cpu().detach())\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    model = model.cpu()\n",
    "    return preds\n",
    "\n",
    "#unic-hard label, Sim\n",
    "def cal_acc(models, i):\n",
    "   \n",
    "    cor_mats = []\n",
    "\n",
    "    train_data = get_dataset('./data/generate_DiffFP_boundary_100.h5')\n",
    "    wrong_indices = list(range(0, 200, 2))  \n",
    "    train_data_subset = Subset(train_data, wrong_indices)\n",
    "    train_loader = DataLoader(train_data_subset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "#\n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        cor_mat = cal_cor_onehot(model, train_loader)\n",
    "        cor_mats.append(cor_mat)\n",
    "\n",
    "    print(len(cor_mats), cor_mat.shape)\n",
    "\n",
    "    baseline = cor_mats[0]\n",
    "\n",
    "    acc = []\n",
    "\n",
    "    for i in range(1, len(cor_mats)):\n",
    "\n",
    "        diff_count = (cor_mats[i] == baseline).sum().item()\n",
    "        acc.append(diff_count/100) \n",
    "\n",
    "\n",
    "    print(acc)\n",
    "  \n",
    "#Tamper detection rate\n",
    "def cal_fragility(models, i):\n",
    "    cor_mats = []\n",
    "    train_data = get_dataset('./data/generate_DiffFP_boundary_100.h5') \n",
    "    wrong_indices = list(range(0, 200, 2)) \n",
    "    train_data_subset = Subset(train_data, wrong_indices)\n",
    "    train_loader = DataLoader(train_data_subset, shuffle=False, batch_size=BATCH_SIZE)   \n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        cor_mat = cal_cor_onehot(model, train_loader)\n",
    "        cor_mats.append(cor_mat)\n",
    "\n",
    "    print(len(cor_mats), cor_mat.shape)\n",
    "    \n",
    "    baseline = cor_mats[0]\n",
    "    differences = []\n",
    "    \n",
    "    for i in range(1, len(cor_mats)):\n",
    "        diff_count = (cor_mats[i] != baseline).sum().item()\n",
    "        differences.append(diff_count)\n",
    "        print(f\"{i}:{diff_count}, diff rate:{diff_count/100}\")\n",
    "    \n",
    "#AUC\n",
    "def cal_correlation(models,i):\n",
    "\n",
    "    cor_mats = []\n",
    "\n",
    "\n",
    "    train_data =get_dataset('./data/generate_DiffFP_boundary_100.h5')\n",
    "    indices =  list(range(0, 200, 2)) #wrong \n",
    "    train_data_subset = Subset(train_data, indices)\n",
    "    train_loader = DataLoader(train_data_subset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(models)):\n",
    "\n",
    "        model = models[i]\n",
    "        cor_mat = cal_cor(model, train_loader)\n",
    "        cor_mats.append(cor_mat)\n",
    "\n",
    "    print(len(cor_mats), cor_mat.shape)\n",
    "    \n",
    "\n",
    "\n",
    "    diff = torch.zeros(len(models))\n",
    "    for i in range(len(models) - 1):\n",
    "        iter = i + 1\n",
    "        diff[i] = torch.sum(torch.abs(cor_mats[iter] - cor_mats[0])) / (cor_mat.shape[0] * cor_mat.shape[1]) #L1\n",
    "    print(cor_mat.shape[0],cor_mat.shape[1])\n",
    "\n",
    "    list1 = diff[:20]\n",
    "    list2 = diff[20:40]\n",
    "    list3 = diff[40:60]\n",
    "    list4 = diff[60:65]\n",
    "    list5 = diff[65:80]\n",
    "    list6_1 = diff[80:90]\n",
    "    list6_2 = diff[90:100]\n",
    "\n",
    "    auc_p = calculate_auc(list1, list3)\n",
    "    auc_l = calculate_auc(list2, list3)\n",
    "    auc_tl = calculate_auc(list4, list3)\n",
    "    auc_finetune_all = calculate_auc(list6_1, list3)\n",
    "    auc_finetune_last = calculate_auc(list6_2, list3)\n",
    "    auc_prune = calculate_auc(list5, list3)\n",
    "\n",
    "    print(\"AUC_Prune:\", auc_prune)\n",
    "\n",
    "    print(\"AUC_MEP:\",auc_p,\"AUC_MEL:\", auc_l)#,\n",
    "    print(\"AUC_Finetune_all:\",auc_finetune_all, \"auc_finetune_last:\", auc_finetune_last)\n",
    "    print(\"AUC_TL:\", auc_tl)\n",
    "    ave = (auc_prune + auc_tl + auc_finetune_all + auc_finetune_last + auc_p + auc_l) / 6.0\n",
    "    print(ave)\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "def reset(cls):\n",
    "    if cls == 'resnet':\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        in_feature = model.fc.in_features\n",
    "        model.fc = torch.nn.Linear(in_feature, 10)\n",
    "    elif cls == \"vgg\":\n",
    "        model = models.vgg13(pretrained=False)\n",
    "        model.features[0] = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        for i, layer in enumerate(model.features):\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                model.features[i] = nn.MaxPool2d(kernel_size=2, stride=1, padding=1)\n",
    "        in_feature = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = torch.nn.Linear(in_feature, 10)\n",
    "\n",
    "    elif cls == 'dense':\n",
    "        model = models.densenet121(pretrained=False)\n",
    "        model.features.conv0 = torch.nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        in_feature = model.classifier.in_features\n",
    "        model.classifier = torch.nn.Linear(in_feature, 10)\n",
    "\n",
    "    elif cls == 'mobile':\n",
    "        model = models.mobilenet_v2(pretrained=False)\n",
    "        model.features[0][0] = torch.nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        in_feature = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = torch.nn.Linear(in_feature, 10)\n",
    "\n",
    "    model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from models import get_model\n",
    "\n",
    "\n",
    "def reset(cls):\n",
    "    if cls == 'resnet':\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    elif cls == 'vgg':\n",
    "        model = models.vgg13(pretrained=False)\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        for i, layer in enumerate(model.features):\n",
    "            if isinstance(layer, nn.MaxPool2d):\n",
    "                model.features[i] = nn.MaxPool2d(kernel_size=2, stride=1, padding=1)\n",
    "        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 10)\n",
    "    elif cls == 'dense':\n",
    "        model = models.densenet121(pretrained=False)\n",
    "        model.features.conv0 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, 10)\n",
    "    elif cls == 'mobile':\n",
    "        model = models.mobilenet_v2(pretrained=False)\n",
    "        model.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        model.classifier[-1] = nn.Linear(model.classifier[-1].in_features, 10)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {cls}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_model(num, mode):\n",
    "    if mode == \"source\":\n",
    "        path = \"./pretrained_models/teacher_model.pth\"\n",
    "        model = get_model(\"lenet\", \"mnist\", False)\n",
    "\n",
    "    elif mode == \"finetune\":\n",
    "        path = f\"./trained_models/finetune/finetune_{num}.pth\"\n",
    "        model = get_model(\"lenet\", \"mnist\", False)\n",
    "\n",
    "    elif mode == \"finetune_fragile\":\n",
    "        path = f\"./trained_models/finetune/finetune_fragile_{num}.pth\"\n",
    "        model = get_model(\"lenet\", \"mnist\", False)\n",
    "\n",
    "    elif mode == \"kd\":\n",
    "        path = f\"./trained_models/kd/student_model_kd_{num}.pth\"\n",
    "        if num < 5:\n",
    "            cls = 'vgg'\n",
    "        elif num < 10:\n",
    "            cls = 'resnet'\n",
    "        elif num < 15:\n",
    "            cls = 'dense'\n",
    "        else:\n",
    "            cls = 'mobile'\n",
    "        model = reset(cls)\n",
    "\n",
    "    elif mode == \"clean\":\n",
    "        path = f\"./trained_models/irrelevant/clean_model_{num}.pth\"\n",
    "        if num < 5:\n",
    "            cls = 'vgg'\n",
    "        elif num < 10:\n",
    "            cls = 'resnet'\n",
    "        elif num < 15:\n",
    "            cls = 'dense'\n",
    "        else:\n",
    "            cls = 'mobile'\n",
    "        model = reset(cls)\n",
    "\n",
    "    elif mode == \"prune\":\n",
    "        path = f\"./trained_models/fine_pruning/prune_model_{num}.pth\"\n",
    "        model = torch.load(path)\n",
    "        return model\n",
    "\n",
    "    elif mode == \"tl\":\n",
    "        path = f\"./trained_models/transfer/tl{num}.pth\"\n",
    "        model = get_model(\"lenet\", \"mnist\", False)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported mode: {mode}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = []\n",
    "\n",
    "for i in [0]:\n",
    "    globals()['teacher' + str(i)] = load_model(i, \"source\")\n",
    "    models_list.append(globals()['teacher' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['mep' + str(i)] = load_model(i, \"kd\") #MEP\n",
    "    models_list.append(globals()['mep' + str(i)])\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['mel' + str(i)] = load_model(i, \"1\")  #MEL\n",
    "    models_list.append(globals()['mel' + str(i)])\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['clean' + str(i)] = load_model(i, \"clean\")\n",
    "    models_list.append(globals()['clean' + str(i)])\n",
    "\n",
    "for i in range(5): #TL\n",
    "    globals()['KMNIST' + str(i)] = load_model(i, \"tl\")\n",
    "    models_list.append(globals()['KMNIST' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    globals()['fp' + str(i)] = load_model(i, \"prune\") #FP\n",
    "    models_list.append(globals()['fp' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['finetune' + str(i)] = load_model(i, 'finetune') #0-10:FT-all, 10-20 FT-last\n",
    "    models_list.append(globals()['finetune' + str(i)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "for i in range(1):\n",
    "    iter = i\n",
    "    print(\"Iter:\", iter)\n",
    "    cal_correlation(models_list,iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fragility Tamper detection rate\n",
    "models_list = []\n",
    "\n",
    "for i in [0]:\n",
    "    globals()['teacher' + str(i)] = load_model(i, \"source\")\n",
    "    models_list.append(globals()['teacher' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['mep' + str(i)] = load_model(i, \"kd\") #MEP\n",
    "    models_list.append(globals()['mep' + str(i)])\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['mel' + str(i)] = load_model(i, \"1\")  #MEL\n",
    "    models_list.append(globals()['mel' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(5): #TL\n",
    "    globals()['KMNIST' + str(i)] = load_model(i, \"tl\")\n",
    "    models_list.append(globals()['KMNIST' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(16):\n",
    "    globals()['fp' + str(i)] = load_model(i, \"prune\") #FP\n",
    "    models_list.append(globals()['fp' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['finetune_fragile' + str(i)] = load_model(i, 'finetune_fragile') #FT-L epoch 0~100\n",
    "    models_list.append(globals()['finetune_fragile' + str(i)])\n",
    "\n",
    "for i in range(1):\n",
    "        iter = i\n",
    "        print(\"Iter:\", iter)\n",
    "        cal_fragility(models_list, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uni-hard_label\n",
    "models_list = []\n",
    "\n",
    "for i in [0]:\n",
    "    globals()['teacher' + str(i)] = load_model(i, \"source\")\n",
    "    models_list.append(globals()['teacher' + str(i)])\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    globals()['clean' + str(i)] = load_model(i, \"clean\")\n",
    "    models_list.append(globals()['clean' + str(i)])\n",
    "\n",
    "    \n",
    "for i in range(1):\n",
    "        iter = i\n",
    "        print(\"Iter:\", iter)\n",
    "        cal_acc(models_list, iter) #unic, similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uni-soft_labels\n",
    "def extract_features(model, inputs, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        outputs = F.softmax(outputs, dim=1)\n",
    "    return outputs.cpu()\n",
    "\n",
    "\n",
    "def distance_function(a, b):\n",
    "    epsilon = 1e-9\n",
    "    a = torch.clamp(a, epsilon, 1)\n",
    "    b = torch.clamp(b, epsilon, 1)\n",
    "    return (a * (a.log() - b.log())).sum(dim=1)\n",
    "\n",
    "\n",
    "def estimate_uniqueness(source_model, fingerprint_dataset, unrelated_models, delta_values, device='cuda'):\n",
    "    fingerprint_loader = DataLoader(fingerprint_dataset, batch_size=100, shuffle=False)\n",
    "    all_fingerprints = torch.cat([x.to(device) for x, _ in fingerprint_loader], dim=0)\n",
    "\n",
    "    fM = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(all_fingerprints), 10):\n",
    "            batch = all_fingerprints[i:i+10]\n",
    "            features = extract_features(source_model, batch, device=device)\n",
    "            fM.append(features)\n",
    "    fM = torch.cat(fM, dim=0)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for delta in delta_values:\n",
    "        min_distances = []\n",
    "        avg_distances = []\n",
    "        collision_count = 0\n",
    "        for umodel in unrelated_models:\n",
    "            fMprime = []\n",
    "            with torch.no_grad():\n",
    "                for i in range(0, len(all_fingerprints), 100):\n",
    "                    batch = all_fingerprints[i:i+100]\n",
    "                    features = extract_features(umodel, batch, device=device)\n",
    "                    fMprime.append(features)\n",
    "            fMprime = torch.cat(fMprime, dim=0)\n",
    "\n",
    "            dist = distance_function(fM, fMprime)\n",
    "            min_dist = dist.min().item()\n",
    "            avg_dist = dist.mean().item()\n",
    "            min_distances.append(min_dist)\n",
    "            avg_distances.append(avg_dist)\n",
    "\n",
    "            if torch.any(dist <= delta):\n",
    "                collision_count += 1\n",
    "\n",
    "        uniqueness = 1 - (collision_count / len(unrelated_models))\n",
    "        results.append((delta.item(), uniqueness, min(min_distances), sum(avg_distances) / len(avg_distances)))\n",
    "        print(f\"Δ={delta:.2f} | Uniqueness={uniqueness:.4f} | MinDist={min(min_distances):.4f} | AvgDist={sum(avg_distances)/len(avg_distances):.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = 'cuda'\n",
    "    delta_values = torch.arange(0, 0.11, 0.01)\n",
    "\n",
    "\n",
    "    source_model = load_model(0, \"source\").to(device)\n",
    "    source_model.eval()\n",
    "\n",
    "\n",
    "    dataset_path = \"./data/generate_DiffFP_boundary_100.h5\"\n",
    "    full_dataset = get_dataset(dataset_path)\n",
    "    fingerprint_dataset = Subset(full_dataset, list(range(0, 200, 2)))  # only x_w\n",
    "\n",
    "    unrelated_models = [load_model(i, \"clean\").to(device).eval() for i in range(20)]\n",
    "\n",
    "    print(\"Evaluating Uniqueness...\")\n",
    "    results = estimate_uniqueness(source_model, fingerprint_dataset, unrelated_models, delta_values, device=device)\n",
    "\n",
    "    print(\"\\nFinal Summary:\")\n",
    "    for delta, uniq, min_dist, avg_dist in results:\n",
    "        print(f\"Δ={delta:.2f} | Uniqueness={uniq:.4f} | MinDist={min_dist:.4f} | AvgDist={avg_dist:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
